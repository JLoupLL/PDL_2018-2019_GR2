Dataset Name;Brief description;Preprocessing;Instances;Format;Default Task;Created (updated);Reference;Creator;
Human Motion DataBase (HMDB51);51 action categories, each containing at least 101 clips, extracted from a range of sources.;None.;6,766 video clips;video clips;Action classification;2011;[47];H. Kuehne et al.;
TV Human Interaction Dataset;Videos from 20 different TV shows for prediction social actions: handshake, high five, hug, kiss and none.;None.;6,766 video clips;video clips;Action prediction;2013;[48];Patron-Perez, A. et al.;
UT Interaction;People acting out one of 6 actions (shake-hands, point, hug, push, kick, and punch) sometimes with multiple groups in the same video clip.;None.;120 video clips;video clips;Action prediction;2009;[49];Ryoo, M. S. et al.;
UT Kinect;10 different people performing one of 6 actions (walk, sit down, stand up, pick up, carry, throw, push, pull, wave hands and clap hands) in an office setting.;None.;200 video clips with depth information at 15 frames per second;video clips with depth information;Action classification;2012;[50];Xia, L. et al.;
SBU Interact;Seven participants performing one of 8 actions together (approaching, departing, pushing, kicking, punching, exchanging objects, hugging, and shaking hands) in an office setting.;None.;Around 300 interactions;video clips with depth information;Action classification;2012;[51];Yun, K. et al.;
Berkeley Multimodal Human Action Database (MHAD);Recordings of a single person performing 12 actions;MoCap pre-processing;660 action samples;8 PhaseSpace Motion Capture, 2 Stereo Cameras, 4 Quad Cameras, 6 accelerometers, 4 microphones;Action classification;2013;[52];Ofli, F. et al.;
UCF 101 Dataset;Self described as "a dataset of 101 human actions classes from videos in the wild." Dataset is large with over 27 hours of video.;Actions classified and labeled.;13,000;Video, images, text;Classification, action detection;2012;[53][54];K. Soomro et al.;
THUMOS Dataset;Large video dataset for action classification.;Actions classified and labeled.;45M frames of video;Video, images, text;Classification, action detection;2013;[55][56];Y. Jiang et al.;
Activitynet;Large video dataset for activity recognition and detection.;Actions classified and labeled.;10,024;Video, images, text;Classification, action detection;2015;[57];Heilbron et al.;
MSP-AVATAR;Improvised scenarios annotated for discourse functions: contrast, confirmation/negation, question, uncertainty, suggest, giving orders, warn, inform, size description, using pronouns.;Actions classified and labeled.;74 sessions;Motion-captured video, audio;Classification, action detection;2015;[58];Sadoughi, N. et al.;
LILiR Twotalk Corpus;Video datasets for non-verbal communication activity recognition: agreement, thinking, asking and understanding.;Actions classified and labeled.;527;Video;Action detection;2011;[59];Sheerman-Chase et al.;
MEXAction2;Video dataset for action localization and spotting;Actions classified and labeled.;1000;Video;Action detection;2014;[60];Stoian et al.;
